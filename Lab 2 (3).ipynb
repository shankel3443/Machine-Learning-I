{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "\n",
    "#### Alan Abadzic, John Girard, Eric Laigaie, Garrett Shankel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"NY_Listings_Validated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation Part 1\n",
    "\n",
    "*Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Grade Variable and Encode it\n",
    "def categorise(row):  \n",
    "    if row['Review Scores Rating'] > 89:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    return 'IDK'\n",
    "\n",
    "df['Grade'] = df.apply(lambda row: categorise(row), axis=1)\n",
    "\n",
    "df['Grade'].value_counts(normalize=True)\n",
    "\n",
    "# Filter to only useful columns\n",
    "data = df[['Host Response Rate', 'Host Is Superhost', 'Host total listings count', 'City', 'Room type',\n",
    "          'Accommodates', 'Bathrooms', 'Bedrooms', 'Price', 'Minimum nights', 'Maximum nights', 'Availability 365',\n",
    "          'Number of reviews', 'Reviews per month', 'Grade']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot Encode\n",
    "city_one_hot = pd.get_dummies(data['City'])\n",
    "room_one_hot = pd.get_dummies(data['Room type'])\n",
    "\n",
    "data = data.drop('City',axis = 1)\n",
    "data = data.drop('Room type',axis = 1)\n",
    "\n",
    "data = data.join(city_one_hot)\n",
    "data = data.join(room_one_hot)\n",
    "\n",
    "\n",
    "# Map boolean to integer\n",
    "data[\"Host Is Superhost\"] = data[\"Host Is Superhost\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "grade = data['Grade']\n",
    "to_scale = data.drop(\"Grade\", axis = 1)\n",
    "cols = to_scale.columns\n",
    "\n",
    "scaled = scaler.fit_transform(to_scale)\n",
    "\n",
    "data = pd.DataFrame(scaled, columns = cols)\n",
    "data['Grade'] = grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Host Response Rate</th>\n",
       "      <th>Host Is Superhost</th>\n",
       "      <th>Host total listings count</th>\n",
       "      <th>Accommodates</th>\n",
       "      <th>Bathrooms</th>\n",
       "      <th>Bedrooms</th>\n",
       "      <th>Price</th>\n",
       "      <th>Minimum nights</th>\n",
       "      <th>Maximum nights</th>\n",
       "      <th>Availability 365</th>\n",
       "      <th>...</th>\n",
       "      <th>Reviews per month</th>\n",
       "      <th>Bronx</th>\n",
       "      <th>Brooklyn</th>\n",
       "      <th>Manhattan</th>\n",
       "      <th>Queens</th>\n",
       "      <th>Staten Island</th>\n",
       "      <th>Entire home/apt</th>\n",
       "      <th>Private room</th>\n",
       "      <th>Shared room</th>\n",
       "      <th>Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043043</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>1.350418e-08</td>\n",
       "      <td>0.756164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072157</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028028</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>1.396984e-08</td>\n",
       "      <td>0.945205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062780</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016343</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.080080</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>1.396984e-08</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156135</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.140140</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>5.234033e-07</td>\n",
       "      <td>0.980822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027313</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.234033e-07</td>\n",
       "      <td>0.986301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150836</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Host Response Rate  Host Is Superhost  Host total listings count  \\\n",
       "0                 1.0                0.0                   0.004086   \n",
       "1                 1.0                0.0                   0.001021   \n",
       "2                 1.0                0.0                   0.016343   \n",
       "3                 0.7                0.0                   0.001021   \n",
       "4                 1.0                0.0                   0.001021   \n",
       "\n",
       "   Accommodates  Bathrooms  Bedrooms     Price  Minimum nights  \\\n",
       "0      0.000000   0.064516  0.000000  0.043043        0.000801   \n",
       "1      0.000000   0.064516  0.000000  0.028028        0.000801   \n",
       "2      0.200000   0.193548  0.076923  0.080080        0.001601   \n",
       "3      0.200000   0.064516  0.000000  0.140140        0.000801   \n",
       "4      0.066667   0.064516  0.000000  0.060060        0.000000   \n",
       "\n",
       "   Maximum nights  Availability 365  ...  Reviews per month  Bronx  Brooklyn  \\\n",
       "0    1.350418e-08          0.756164  ...           0.072157    1.0       0.0   \n",
       "1    1.396984e-08          0.945205  ...           0.062780    1.0       0.0   \n",
       "2    1.396984e-08          0.972603  ...           0.156135    1.0       0.0   \n",
       "3    5.234033e-07          0.980822  ...           0.027313    1.0       0.0   \n",
       "4    5.234033e-07          0.986301  ...           0.150836    1.0       0.0   \n",
       "\n",
       "   Manhattan  Queens  Staten Island  Entire home/apt  Private room  \\\n",
       "0        0.0     0.0            0.0              0.0           1.0   \n",
       "1        0.0     0.0            0.0              0.0           1.0   \n",
       "2        0.0     0.0            0.0              0.0           1.0   \n",
       "3        0.0     0.0            0.0              1.0           0.0   \n",
       "4        0.0     0.0            0.0              0.0           1.0   \n",
       "\n",
       "   Shared room  Grade  \n",
       "0          0.0      1  \n",
       "1          0.0      0  \n",
       "2          0.0      1  \n",
       "3          0.0      0  \n",
       "4          0.0      1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation Part 2\n",
    "\n",
    "*Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).*\n",
    "\n",
    "Below is table that describes each variable in the dataset. This includes our two predictor variables, Price and Grade. Additionally, all variables are of type float64, except for the int64 Grade.\n",
    "\n",
    "#### Variable Table\n",
    "| Variable | Description |\n",
    "| :-- | :-- |\n",
    "| Host Response Rate | The rate that hosts respond to potential customers. |\n",
    "| Host total listings count | The total number of airbnbs rentals the host owns. |\n",
    "| Accommodates | The number of people this rental can accommodate. |\n",
    "| Bathrooms | The number of bathrooms in this rental. |\n",
    "| Bedrooms | The number of bedrooms in this rental. |\n",
    "| Price | The base price for one night in this rental. |\n",
    "| Minimum nights | The lowest amount of nights this rental can be booked for. |\n",
    "| Maximum nights | The highest amount of nights this rental can be booked for. |\n",
    "| Availability 365 | The proportion of nights in a year that the rental is available. |\n",
    "| Reviews per month | The average number of reviews a rental receives in a month. |\n",
    "| Host Is Superhost | One-hot variable: A \"1\" indicates the rental is operated by a Superhost. |\n",
    "| Bronx | One-hot variable: A \"1\" indicates a Bronx-based rental. |\n",
    "| Brooklyn | One-hot variable: A \"1\" indicates a Brooklyn-based rental. |\n",
    "| Manhattan | One-hot variable: A \"1\" indicates a Manhattan-based rental. |\n",
    "| Queens | One-hot variable: A \"1\" indicates a Queens-based rental. |\n",
    "| Staten Island | One-hot variable: A \"1\" indicates a Staten Island-based rental. |\n",
    "| Entire home/apt | One-hot variable: A \"1\" indicates this rental is an entire home or apartment. |\n",
    "| Private room | One-hot variable: A \"1\" indicates this rental is a private room. |\n",
    "| Shared room | One-hot variable: A \"1\" indicates this rental is a shared room. |\n",
    "| Grade | One-hot variable: A \"1\" indicates this rental has a rating of .9 or more. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and Evaluation 1\n",
    "\n",
    "Choose and explain your evaluation metrics that you will use (i.e., accuracy,\n",
    "precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.\n",
    "\n",
    "#### Grade\n",
    "OPTION A:\n",
    "To measure a model's performance when predicting Grade classification, we will use an F1 score. We decided on this for a few reasons. First, there isn't a large difference in costs between a false negative and false positive. In the case of a false negative, customers will likely get a rental that exceeds their expectations. For a false positive, customers will have to deal with a worse rental than they might be expecting, but this rental could still has a passing review grade (70+). Additionally, the inbalanced class distribution (67% A's, 33% Non-A's) leads us to use F1 instead of plain accuracy due to F1's strength when dealing with unbalanced classes.\n",
    "\n",
    "OPTION B:\n",
    "To measure a model's performance when predicting Grade classification, we will use a sensitivity score. This is mainly due to the potential of a large negative costs for false positives. When a rental is given a false positive prediction, customers could be dealing with a rental that drastically fails their expectations.\n",
    "\n",
    "#### Price\n",
    "To measure a model's performance when predicting Price, we will use mean absolute percent error (MAPE). This option works well because it preserves the scale of the data (as opposed to squaring the difference), and provides a percentage value that is easier to interpret than a dollar value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and Evaluation 2\n",
    "\n",
    "Choose the method you will use for dividing your data into training and\n",
    "testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why\n",
    "your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.\n",
    "\n",
    "#### Grade\n",
    "On top of an 80% train, 20% test split, we will be using stratified 10-fold cross validation with grid search here. While non-stratified CV could also work, our imbalanced dataset (67% A's, 33% Non-A's) could yield folds of data that have very little Non-A's. Using stratification avoids this by holding these proportions constant across folds, leading to better training sets.\n",
    "\n",
    "#### Price\n",
    "On top of an 80% train, 20% test split, we will be using 10-fold cross validation with grid search here. We will not be using stratification because we're not dealing with inbalanced class labels that should be keep consistent. Since our response is continuous, we do not necessarily have to worry about certain outcomes not being selection for each fold.\n",
    "\n",
    "In both cases, grid search helps us determine the best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and Evaluation 3\n",
    "\n",
    "Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "\n",
    "# Split data into train and test\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "train_y = train['Grade']\n",
    "train_x = train.drop('Grade', axis=1)\n",
    "test_y = train['Grade']\n",
    "test_x = train.drop('Grade', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grade : Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Random Forest: 88.57%\n",
      "Time Elapsed: 17.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Make classifier\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "# Parameter Grid\n",
    "parameters = {'max_depth':(5, 10), 'min_samples_split':(2, 5, 10), 'splitter':('best', 'random')}\n",
    "\n",
    "# Make GridSearch - inputting a cv value toggles stratification on\n",
    "dt_cv = GridSearchCV(estimator = dt_clf, param_grid = parameters, cv = 10)\n",
    "\n",
    "# Fit\n",
    "start = time.time()\n",
    "dt_cv.fit(train_x,train_y)\n",
    "end = time.time()\n",
    "\n",
    "# Find best parameters and predict on them\n",
    "best_parameters = dt_cv.best_params_\n",
    "preds = dt_cv.predict(test_x)\n",
    "print(\"F1 Score for Random Forest: \" + str(round(f1_score(test_y, preds)*100,2)) + \"%\")\n",
    "print(\"Time Elapsed: \" + str(round(end-start,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grade : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Logistic Regression: 78.35\n",
      "Time Elapsed: 200.06\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_clf = LogisticRegression(max_iter = 100000)\n",
    "\n",
    "parameters = {'C':(1,10,100), 'solver':('lbfgs','saga')}\n",
    "lr_cv = GridSearchCV(estimator = lr_clf, param_grid = parameters, cv = 10)\n",
    "start = time.time()\n",
    "lr_cv.fit(train_x,train_y)\n",
    "end = time.time()\n",
    "\n",
    "best_parameters = lr_cv.best_params_\n",
    "preds = lr_cv.predict(test_x)\n",
    "print(\"F1 Score for Logistic Regression: \" + str(round(f1_score(test_y, preds)*100,2)) + \"%\")\n",
    "print(\"Time Elapsed: \" + str(round(end-start,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grade : KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for KNN: 85.26\n",
      "Time Elapsed: 357.3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "parameters = {'n_neighbors':(3,5,7), 'algorithm':('auto','kd_tree')}\n",
    "knn_cv = GridSearchCV(estimator = knn_clf, param_grid = parameters, cv = 10)\n",
    "start = time.time()\n",
    "knn_cv.fit(train_x,train_y)\n",
    "end = time.time()\n",
    "\n",
    "best_parameters = knn_cv.best_params_\n",
    "preds = knn_cv.predict(test_x)\n",
    "print(\"F1 Score for KNN: \" + str(round(f1_score(test_y, preds)*100,2)) + \"%\")\n",
    "print(\"Time Elapsed: \" + str(round(end-start,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Price column\n",
    "data['Price'] = df['Price']\n",
    "\n",
    "# Split data into train and test\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "train_y = train['Price']\n",
    "train_x = train.drop('Price', axis=1)\n",
    "test_y = train['Price']\n",
    "test_x = train.drop('Price', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price : Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percent Error for Linear Regression: 39.3%\n",
      "Time Elapsed: 1.38\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.metrics import mean_absolute_error\n",
    "linr_clf = LinearRegression()\n",
    "\n",
    "parameters = {'fit_intercept':(True,False)}\n",
    "linr_cv = GridSearchCV(estimator = linr_clf, param_grid = parameters, cv = 10)\n",
    "start = time.time()\n",
    "linr_cv.fit(train_x,train_y)\n",
    "end = time.time()\n",
    "\n",
    "best_parameters = linr_cv.best_params_\n",
    "preds = linr_cv.predict(test_x)\n",
    "check_MAPE = pd.DataFrame()\n",
    "check_MAPE['real'], check_MAPE['preds'] = test_y, preds\n",
    "check_MAPE['diff'] = abs(check_MAPE['preds'] - check_MAPE['real']) / check_MAPE['real']\n",
    "check_MAPE = check_MAPE.replace(np.inf, np.nan)\n",
    "print(\"Mean Absolute Percent Error for Linear Regression: \" + str(round((check_MAPE['diff'].mean())*100,2)) + '%')\n",
    "print(\"Time Elapsed: \" + str(round(end-start,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price : Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percent Error for Elastic Net Regression: 41.29%\n",
      "Time Elapsed: 13.26\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "el_clf = ElasticNet()\n",
    "\n",
    "parameters = {'l1_ratio':(.25,.5,.75),'alpha':(.1,1,10),'selection':('random','cyclic')}\n",
    "el_cv = GridSearchCV(estimator = el_clf, param_grid = parameters, cv = 10)\n",
    "start = time.time()\n",
    "el_cv.fit(train_x,train_y)\n",
    "end = time.time()\n",
    "\n",
    "best_parameters = el_cv.best_params_\n",
    "preds = el_cv.predict(test_x)\n",
    "check_MAPE = pd.DataFrame()\n",
    "check_MAPE['real'], check_MAPE['preds'] = test_y, preds\n",
    "check_MAPE['diff'] = abs(check_MAPE['preds'] - check_MAPE['real']) / check_MAPE['real']\n",
    "check_MAPE = check_MAPE.replace(np.inf, np.nan)\n",
    "print(\"Mean Absolute Percent Error for Elastic Net Regression: \" + str(round((check_MAPE['diff'].mean())*100,2)) + '%')\n",
    "print(\"Time Elapsed: \" + str(round(end-start,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price : Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percent Error for SVR: 67.42%\n",
      "Time Elapsed: 408.68\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_clf = RandomForestRegressor()\n",
    "\n",
    "parameters = {'max_depth':(5, 10), 'bootstrap':(True, False)}\n",
    "rf_cv = GridSearchCV(estimator = rf_clf, param_grid = parameters, cv = 10)\n",
    "start = time.time()\n",
    "rf_cv.fit(train_x,train_y)\n",
    "end = time.time()\n",
    "\n",
    "best_parameters = rf_cv.best_params_\n",
    "preds = rf_cv.predict(test_x)\n",
    "check_MAPE = pd.DataFrame()\n",
    "check_MAPE['real'], check_MAPE['preds'] = test_y, preds\n",
    "check_MAPE['diff'] = abs(check_MAPE['preds'] - check_MAPE['real']) / check_MAPE['real']\n",
    "check_MAPE = check_MAPE.replace(np.inf, np.nan)\n",
    "print(\"Mean Absolute Percent Error for Random Forest Regression: \" + str(round((1 - check_MAPE['diff'].mean())*100,2)) + '%')\n",
    "print(\"Time Elapsed: \" + str(round(end-start,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and Evaluation 4\n",
    "\n",
    "Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "\n",
    "SHOW MODEL ERROR/ACCURACY ON CHART WITH CONFIDENCE INTERVAL BARS? THAT WAY YOU COULD COMPARE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and Evaluation 5\n",
    "\n",
    "Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniquesâ€”be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.\n",
    "\n",
    "NOT SURE WHAT THEY'RE TALKING ABOUT WITH STATISTICAL TESTS TO COMPARE PERFORMANCE, I'LL HAVE TO GO LOOK THAT UP. BUT WE CAN SPEAK TO ADVANTAGES LIKE INTERPRETABILITY / TIME / ACCURACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and Evaluation 6\n",
    "\n",
    "Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.\n",
    "\n",
    "WE DID SOMETHING VERY SIMILAR IN THE MINILAB. JUST NEED TO DO IT FOR THE BEST PRICE AND GRADE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "Our model is using best used to find a price point that would involve finding either an under or overvalued price point for rentals.\n",
    "\n",
    "To find these price points we could use to evaluvate the true value of a listing. When using our models we could figure out if a listing is overpriced for the area it is in or if it is a value find. With the rising costs of Airbnb's lately, and people turning back to using hotels, the owners of the rentals will want to ensure that their listings are generating revenue and not being ignored due to bad pricing. This will serve the owner of the property immensely, because having an extra mortgage that is not being subsidized by income from renters can prove to be quite perilous. \n",
    "\n",
    "For the end user who wants the convenience of having \n",
    "\n",
    "#### How to deploy this model within the application\n",
    "We feel that once we had a lock on what the pricing point could/should be, the lister of the rental would be able to select a price using a sliding scale(drag along a line to select a price). Airbnb could use this scale in this manner: \n",
    "\n",
    "A starting point on one side of the scale(left) where the price is lower so it would greatly appeal to users looking for a value listing.\n",
    "On the other side of the scale(right) have a higher price point listed that would garner more profit to the lister. \n",
    "Ultimately it will be up to the lister of the property to select a price point that appeals to potential renters, we are simply giving them a good and believable spot to list their property at. \n",
    "\n",
    "If used propely, there could be an equilibrium met where the renter feels they are getting a fair price, and the lister feels that their property is fetching a fair market value.\n",
    "\n",
    "#### What other data needs to be collected\n",
    "Since our model is pretty heavily aimed at price, I think a good data point that would need to be collected would be the neighborghood. The price of the listing would probably be closely correlated to the neighborhood. People will pay more/less to stay at a listing in certain neighborhoods, so our pricing model needs to be looked at in the confines of what neighborhood the listing is in. I.E accurate pricing within each neighborhood.\n",
    "\n",
    "One other big data point I think we should capture is how often listings are actually being used (the % of listings that are being used/the total amount of listings available). New York for example often has weekends where more people are staying in rental properties (Fashion Week, Tennis' US Open, NYC marathon, Super Bowl, etc..). Weekends like this are bound to capture more out of towners seeking temporary housing accomodations. Having an increase in demand will allow listers of properties to be able to charge more money for their listings.\n",
    "\n",
    "#### How frequnetly should our data be updated\n",
    "Paying attention to real estate is always a very active endeavor. With the nature of real estate being what it is, I feel that our data would need to be updated weekly. However, when there is an anticipated rise in demand of properties like some of the example listed above, it would probably be a strong idea to update it on a daily basis. Having a accurate pricing point during these demand increases will allow the lister to feel that they are not losing out on potential money."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exceptional Work\n",
    "\n",
    "You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
