{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Models (50 POINTS)\n",
    "Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe. \n",
    "\n",
    "#### Model Advantages (10 POINTS)\n",
    "Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail.\n",
    "\n",
    "#### Interpret Feature Importance (30 POINTS)\n",
    "Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?\n",
    "\n",
    "#### Interpret Support Vectors (10 POINTS)\n",
    "Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC modelâ€” then analyze the support vectors from the subsampled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Lab\n",
    "\n",
    "#### Alan Abadzic, John Girard, Eric Laigaie, Garrett Shankel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"NY_Listings_Validated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A        27589\n",
       "Not A    16436\n",
       "Name: Grade, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we need to create our classification column. Any rentals with a Review Scores Rating above 90 receive \"A\", else\n",
    "# recieve \"Not A\".\n",
    "\n",
    "def categorise(row):  \n",
    "    if row['Review Scores Rating'] > 89:\n",
    "        return 'A'\n",
    "    else:\n",
    "        return 'Not A'\n",
    "    return 'IDK'\n",
    "\n",
    "df['Grade'] = df.apply(lambda row: categorise(row), axis=1)\n",
    "\n",
    "#df['Grade'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only useful columns\n",
    "data = df[['Host Response Rate', 'Host Is Superhost', 'Host total listings count', 'City', 'Room type',\n",
    "          'Accommodates', 'Bathrooms', 'Bedrooms', 'Price', 'Minimum nights', 'Maximum nights', 'Availability 365',\n",
    "          'Number of reviews', 'Reviews per month', 'Grade']]\n",
    "\n",
    "\n",
    "# One-hot Encode\n",
    "city_one_hot = pd.get_dummies(data['City'])\n",
    "room_one_hot = pd.get_dummies(data['Room type'])\n",
    "\n",
    "data = data.drop('City',axis = 1)\n",
    "data = data.drop('Room type',axis = 1)\n",
    "\n",
    "data = data.join(city_one_hot)\n",
    "data = data.join(room_one_hot)\n",
    "\n",
    "\n",
    "# Map boolean to integer\n",
    "data[\"Host Is Superhost\"] = data[\"Host Is Superhost\"].astype(int)\n",
    "\n",
    "\n",
    "# Scale Data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "grade = data['Grade']\n",
    "to_scale = data.drop(\"Grade\", axis = 1)\n",
    "\n",
    "data = scaler.fit_transform(to_scale)\n",
    "data = pd.DataFrame(data)\n",
    "data['Grade'] = grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 35220, Test: 8805\n"
     ]
    }
   ],
   "source": [
    "# Split into 80/20 train/test split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "\n",
    "print(\"Train: \" + str(len(train)) + \", Test: \" + str(len(test)))\n",
    "\n",
    "train_y = train['Grade']\n",
    "train_x = train.drop('Grade', axis=1)\n",
    "test_y = train['Grade']\n",
    "test_x = train.drop('Grade', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing penalties with saga solver\n",
      "None: 71.41%\n",
      "l1: 71.37%\n",
      "l2: 71.35%\n",
      "Elastic Net: 71.5%\n",
      "\n",
      "Testing C values with Elastic Net penalty\n",
      "0.05: 70.44%\n",
      "0.5: 71.41%\n",
      "1: 71.5%\n",
      "100: 71.4%\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Testing different penalties with saga\n",
    "print(\"Testing penalties with saga solver\")\n",
    "clf = LogisticRegression(max_iter=1000, penalty='none', solver='saga').fit(train_x, train_y)\n",
    "predictions = clf.predict(test_x)\n",
    "acc = round((mt.accuracy_score(predictions, test_y)) * 100,2)\n",
    "print('None: ' + str(acc) + '%')\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, penalty='l1', solver='saga').fit(train_x, train_y)\n",
    "predictions = clf.predict(test_x)\n",
    "acc = round((mt.accuracy_score(predictions, test_y)) * 100,2)\n",
    "print('l1: ' + str(acc) + '%')\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, penalty='l2', solver='saga').fit(train_x, train_y)\n",
    "predictions = clf.predict(test_x)\n",
    "acc = round((mt.accuracy_score(predictions, test_y)) * 100,2)\n",
    "print('l2: ' + str(acc) + '%')\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, penalty='elasticnet', solver='saga', l1_ratio=0.5).fit(train_x, train_y)\n",
    "predictions = clf.predict(test_x)\n",
    "acc = round((mt.accuracy_score(predictions, test_y)) * 100,2)\n",
    "print('Elastic Net: ' + str(acc) + '%' + '\\n')\n",
    "\n",
    "print('Testing C values with Elastic Net penalty')\n",
    "clf = LogisticRegression(max_iter=1000, penalty='elasticnet', solver='saga', l1_ratio=0.5, C=0.05).fit(train_x, train_y)\n",
    "predictions = clf.predict(test_x)\n",
    "acc = round((mt.accuracy_score(predictions, test_y)) * 100,2)\n",
    "print('0.05: ' + str(acc) + '%')\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, penalty='elasticnet', solver='saga', l1_ratio=0.5, C=.5).fit(train_x, train_y)\n",
    "predictions = clf.predict(test_x)\n",
    "acc = round((mt.accuracy_score(predictions, test_y)) * 100,2)\n",
    "print('0.5: ' + str(acc) + '%')\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1).fit(train_x, train_y)\n",
    "predictions = clf.predict(test_x)\n",
    "acc = round((mt.accuracy_score(predictions, test_y)) * 100,2)\n",
    "print('1: ' + str(acc) + '%')\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, penalty='elasticnet', solver='saga', l1_ratio=0.5, C=100).fit(train_x, train_y)\n",
    "predictions = clf.predict(test_x)\n",
    "acc = round((mt.accuracy_score(predictions, test_y)) * 100,2)\n",
    "print('100: ' + str(acc) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-124-aca92513dac2>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-124-aca92513dac2>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    #print(name, 'has weight of', coef[0])\u001b[0m\n\u001b[1;37m                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(max_iter=1000, penalty='elasticnet', solver='saga', l1_ratio=0.5).fit(train_x, train_y)\n",
    "predictions = lr_clf.predict(test_x)\n",
    "acc = round((mt.accuracy_score(predictions, test_y)) * 100,2)\n",
    "conf = mt.confusion_matrix(predictions, test_y)\n",
    "\n",
    "\n",
    "weights = lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = data.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    #print(name, 'has weight of', coef[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6311186825667234\n",
      "[[21992 12912]\n",
      " [   80   236]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "svm_clf = SVC(C=0.5, kernel='rbf', degree=3, gamma='auto').fit(train_x, train_y)\n",
    "predictions = svm_clf.predict(test_x)\n",
    "acc = mt.accuracy_score(predictions, test_y)\n",
    "print('accuracy:', acc )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
